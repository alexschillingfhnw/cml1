{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cml1 - Immobilienrechner\n",
    "# 2.2 Bestmögliches Regressionsmodell - kaggle-Contest\n",
    "\n",
    "Entwickle mit beliebigen Algorithmen das bestmögliche Modell im Sinne des Mean absolute percentage error (MAPE). Vergleiche dabei mindestens drei algorithmische Ansätze, wobei ein multiples lineares Modell Teil davon sein soll als Benchmark. Untersuche die ‘Variable Importance’ für dein bestes Modell.\n",
    "\n",
    "Abgabe\n",
    "\n",
    "Notebook und daraus erstellter Bericht (ohne Code) als pdf, welche die Entwicklung deines besten Modells, sowie der zwei weiteren Modelle dokumentiert, inklusive verwendeter Features, Preprocessing, Model Selection Prozess und Untersuchung der ‘Variable Importance’.\n",
    "\n",
    "Eingabe der Vorhersage des Preises für den Testdatensatz mit deinem bestmöglichen Modell auf kaggle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# linear models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "\n",
    "# tree models\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# ensemble models\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "# xgb model\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# imputing\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OneHotEncoder\n",
    "\n",
    "# pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# model selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, RandomizedSearchCV\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
    "\n",
    "# save model\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten laden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit die Predictions die korrekten IDs haben, müssen die rohe Kaggle-Daten geladen werden und die IDs extrahiert werden. Diese werden wir später wieder benötigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_kaggle = pd.read_csv('../../../data/test_data-Kaggle-v0.11.csv', low_memory=False)\n",
    "\n",
    "# get indexes for predictions\n",
    "indexes = df_raw_kaggle[\"Unnamed: 0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price_cleaned</th>\n",
       "      <th>kanton</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Year built:</th>\n",
       "      <th>type_unified</th>\n",
       "      <th>Availability_Categorized</th>\n",
       "      <th>Living_area_unified</th>\n",
       "      <th>Floor_unified</th>\n",
       "      <th>Floor_space_merged</th>\n",
       "      <th>Plot_area_unified</th>\n",
       "      <th>...</th>\n",
       "      <th>PopulationDensityS</th>\n",
       "      <th>RiversAndLakesL</th>\n",
       "      <th>RiversAndLakesM</th>\n",
       "      <th>RiversAndLakesS</th>\n",
       "      <th>WorkplaceDensityL</th>\n",
       "      <th>WorkplaceDensityM</th>\n",
       "      <th>WorkplaceDensityS</th>\n",
       "      <th>ForestDensityL</th>\n",
       "      <th>ForestDensityM</th>\n",
       "      <th>ForestDensityS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1150000.0</td>\n",
       "      <td>AG</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>penthouse</td>\n",
       "      <td>On request</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366674</td>\n",
       "      <td>0.082170</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.011871</td>\n",
       "      <td>0.030169</td>\n",
       "      <td>0.052120</td>\n",
       "      <td>0.098951</td>\n",
       "      <td>0.511176</td>\n",
       "      <td>0.286451</td>\n",
       "      <td>0.090908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1420000.0</td>\n",
       "      <td>AG</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>terrace-house</td>\n",
       "      <td>On request</td>\n",
       "      <td>156.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>242.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366674</td>\n",
       "      <td>0.082170</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.011871</td>\n",
       "      <td>0.030169</td>\n",
       "      <td>0.052120</td>\n",
       "      <td>0.098951</td>\n",
       "      <td>0.511176</td>\n",
       "      <td>0.286451</td>\n",
       "      <td>0.090908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>720000.0</td>\n",
       "      <td>AG</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>penthouse</td>\n",
       "      <td>Immediately</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.635194</td>\n",
       "      <td>0.154274</td>\n",
       "      <td>0.188229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172646</td>\n",
       "      <td>0.163850</td>\n",
       "      <td>0.165830</td>\n",
       "      <td>0.163362</td>\n",
       "      <td>0.095877</td>\n",
       "      <td>0.001911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1430000.0</td>\n",
       "      <td>AG</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>detached-house</td>\n",
       "      <td>On request</td>\n",
       "      <td>154.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>257.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366674</td>\n",
       "      <td>0.082170</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.011871</td>\n",
       "      <td>0.030169</td>\n",
       "      <td>0.052120</td>\n",
       "      <td>0.098951</td>\n",
       "      <td>0.511176</td>\n",
       "      <td>0.286451</td>\n",
       "      <td>0.090908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>995000.0</td>\n",
       "      <td>AG</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>flat</td>\n",
       "      <td>On request</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204549</td>\n",
       "      <td>0.109586</td>\n",
       "      <td>0.141473</td>\n",
       "      <td>0.091805</td>\n",
       "      <td>0.046950</td>\n",
       "      <td>0.038008</td>\n",
       "      <td>0.055509</td>\n",
       "      <td>0.333865</td>\n",
       "      <td>0.279276</td>\n",
       "      <td>0.145835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   price_cleaned kanton  Zip  Year built:    type_unified  \\\n",
       "0      1150000.0     AG   50          NaN       penthouse   \n",
       "1      1420000.0     AG   50          NaN   terrace-house   \n",
       "2       720000.0     AG   50          NaN       penthouse   \n",
       "3      1430000.0     AG   50          NaN  detached-house   \n",
       "4       995000.0     AG   50          NaN            flat   \n",
       "\n",
       "  Availability_Categorized  Living_area_unified  Floor_unified  \\\n",
       "0               On request                100.0            4.0   \n",
       "1               On request                156.0            NaN   \n",
       "2              Immediately                 93.0            2.0   \n",
       "3               On request                154.0            NaN   \n",
       "4               On request                142.0            0.0   \n",
       "\n",
       "   Floor_space_merged  Plot_area_unified  ...  PopulationDensityS  \\\n",
       "0                 NaN                NaN  ...            0.366674   \n",
       "1               242.0              222.0  ...            0.366674   \n",
       "2                 NaN                NaN  ...            0.635194   \n",
       "3               257.0              370.0  ...            0.366674   \n",
       "4                 NaN                NaN  ...            0.204549   \n",
       "\n",
       "   RiversAndLakesL  RiversAndLakesM  RiversAndLakesS  WorkplaceDensityL  \\\n",
       "0         0.082170         0.001811         0.011871           0.030169   \n",
       "1         0.082170         0.001811         0.011871           0.030169   \n",
       "2         0.154274         0.188229         0.000000           0.172646   \n",
       "3         0.082170         0.001811         0.011871           0.030169   \n",
       "4         0.109586         0.141473         0.091805           0.046950   \n",
       "\n",
       "   WorkplaceDensityM  WorkplaceDensityS  ForestDensityL  ForestDensityM  \\\n",
       "0           0.052120           0.098951        0.511176        0.286451   \n",
       "1           0.052120           0.098951        0.511176        0.286451   \n",
       "2           0.163850           0.165830        0.163362        0.095877   \n",
       "3           0.052120           0.098951        0.511176        0.286451   \n",
       "4           0.038008           0.055509        0.333865        0.279276   \n",
       "\n",
       "   ForestDensityS  \n",
       "0        0.090908  \n",
       "1        0.090908  \n",
       "2        0.001911  \n",
       "3        0.090908  \n",
       "4        0.145835  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../../data/immo_data_clean.csv', low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kanton</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Year built:</th>\n",
       "      <th>type_unified</th>\n",
       "      <th>Availability_Categorized</th>\n",
       "      <th>Living_area_unified</th>\n",
       "      <th>Floor_unified</th>\n",
       "      <th>Floor_space_merged</th>\n",
       "      <th>Plot_area_unified</th>\n",
       "      <th>Rooms_new</th>\n",
       "      <th>...</th>\n",
       "      <th>PopulationDensityS</th>\n",
       "      <th>RiversAndLakesL</th>\n",
       "      <th>RiversAndLakesM</th>\n",
       "      <th>RiversAndLakesS</th>\n",
       "      <th>WorkplaceDensityL</th>\n",
       "      <th>WorkplaceDensityM</th>\n",
       "      <th>WorkplaceDensityS</th>\n",
       "      <th>ForestDensityL</th>\n",
       "      <th>ForestDensityM</th>\n",
       "      <th>ForestDensityS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AG</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>villa</td>\n",
       "      <td>On request</td>\n",
       "      <td>220.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>733.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205251</td>\n",
       "      <td>0.020765</td>\n",
       "      <td>0.034714</td>\n",
       "      <td>0.051031</td>\n",
       "      <td>0.097870</td>\n",
       "      <td>0.103867</td>\n",
       "      <td>0.038822</td>\n",
       "      <td>0.164382</td>\n",
       "      <td>0.100030</td>\n",
       "      <td>0.063548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AG</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>detached-house</td>\n",
       "      <td>On request</td>\n",
       "      <td>230.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>702.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194898</td>\n",
       "      <td>0.017035</td>\n",
       "      <td>0.033235</td>\n",
       "      <td>0.053474</td>\n",
       "      <td>0.064024</td>\n",
       "      <td>0.021157</td>\n",
       "      <td>0.034310</td>\n",
       "      <td>0.260855</td>\n",
       "      <td>0.170434</td>\n",
       "      <td>0.083253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AG</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stepped-house</td>\n",
       "      <td>On request</td>\n",
       "      <td>131.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287587</td>\n",
       "      <td>0.127272</td>\n",
       "      <td>0.067030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032232</td>\n",
       "      <td>0.076203</td>\n",
       "      <td>0.153552</td>\n",
       "      <td>0.434114</td>\n",
       "      <td>0.357984</td>\n",
       "      <td>0.125505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AG</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>terrace-house</td>\n",
       "      <td>Immediately</td>\n",
       "      <td>140.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>140.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.422582</td>\n",
       "      <td>0.024733</td>\n",
       "      <td>0.030412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111079</td>\n",
       "      <td>0.207175</td>\n",
       "      <td>0.167494</td>\n",
       "      <td>0.148190</td>\n",
       "      <td>0.076610</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AG</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>terrace-house</td>\n",
       "      <td>On request</td>\n",
       "      <td>156.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>242.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366674</td>\n",
       "      <td>0.082170</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.011871</td>\n",
       "      <td>0.030169</td>\n",
       "      <td>0.052120</td>\n",
       "      <td>0.098951</td>\n",
       "      <td>0.511176</td>\n",
       "      <td>0.286451</td>\n",
       "      <td>0.090908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  kanton  Zip  Year built:    type_unified Availability_Categorized  \\\n",
       "0     AG   50          NaN           villa               On request   \n",
       "1     AG   50          NaN  detached-house               On request   \n",
       "2     AG   50          NaN   stepped-house               On request   \n",
       "3     AG   50          NaN   terrace-house              Immediately   \n",
       "4     AG   50          NaN   terrace-house               On request   \n",
       "\n",
       "   Living_area_unified  Floor_unified  Floor_space_merged  Plot_area_unified  \\\n",
       "0                220.0            NaN                 NaN              733.0   \n",
       "1                230.0            NaN                 NaN              702.0   \n",
       "2                131.0            NaN                 NaN                NaN   \n",
       "3                140.0            NaN               140.0              206.0   \n",
       "4                156.0            NaN               242.0              222.0   \n",
       "\n",
       "   Rooms_new  ...  PopulationDensityS  RiversAndLakesL  RiversAndLakesM  \\\n",
       "0        6.5  ...            0.205251         0.020765         0.034714   \n",
       "1        7.5  ...            0.194898         0.017035         0.033235   \n",
       "2        4.5  ...            0.287587         0.127272         0.067030   \n",
       "3        6.5  ...            0.422582         0.024733         0.030412   \n",
       "4        4.5  ...            0.366674         0.082170         0.001811   \n",
       "\n",
       "   RiversAndLakesS  WorkplaceDensityL  WorkplaceDensityM  WorkplaceDensityS  \\\n",
       "0         0.051031           0.097870           0.103867           0.038822   \n",
       "1         0.053474           0.064024           0.021157           0.034310   \n",
       "2         0.000000           0.032232           0.076203           0.153552   \n",
       "3         0.000000           0.111079           0.207175           0.167494   \n",
       "4         0.011871           0.030169           0.052120           0.098951   \n",
       "\n",
       "   ForestDensityL  ForestDensityM  ForestDensityS  \n",
       "0        0.164382        0.100030        0.063548  \n",
       "1        0.260855        0.170434        0.083253  \n",
       "2        0.434114        0.357984        0.125505  \n",
       "3        0.148190        0.076610        0.000000  \n",
       "4        0.511176        0.286451        0.090908  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kaggle = pd.read_csv('../../../data/test_data_kaggle_clean.csv', low_memory=False)\n",
    "df_kaggle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove cols Year built:, Availability_Categorized\n",
    "#df = df.drop(columns=[\"Year built:\", \"Availability_Categorized\"])\n",
    "#df_kaggle = df_kaggle.drop(columns=[\"Year built:\", \"Availability_Categorized\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after removing rows with missing target values:\n",
      "(21381, 99)\n",
      "Kaggle Test Dataset\n",
      "(24556, 99)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_dataframe(df):\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    df_processed = pd.get_dummies(df, columns=categorical_columns, drop_first=True, dtype=int)\n",
    "\n",
    "    return df_processed, numerical_columns\n",
    "\n",
    "\n",
    "def remove_outliers(df, numerical_columns):\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    quantiles_1 = df_processed[numerical_columns].quantile(0.002)\n",
    "    quantiles_99 = df_processed[numerical_columns].quantile(0.998)\n",
    "\n",
    "    for column in numerical_columns:\n",
    "        condition = (df_processed[column] < quantiles_1[column]) | (df_processed[column] > quantiles_99[column])\n",
    "        df_processed.loc[condition, column] = None\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "df, num_cols_df = preprocess_dataframe(df)\n",
    "df_kaggle, num_cols_df_kaggle = preprocess_dataframe(df_kaggle)\n",
    "\n",
    "df = remove_outliers(df, num_cols_df)\n",
    "\n",
    "df = df[df['price_cleaned'] > 10000]\n",
    "\n",
    "df = df.dropna(subset=['price_cleaned'], axis=0)\n",
    "\n",
    "print(\"Dataset after removing rows with missing target values:\")\n",
    "print(df.shape)\n",
    "\n",
    "print(\"Kaggle Test Dataset\")\n",
    "print(df_kaggle.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframes do not have the same column names.\n",
      "Columns in df1 not in df2: {'price_cleaned'}\n",
      "Columns in df2 not in df1 (dropped): {'type_unified_furnished-residential-property'}\n"
     ]
    }
   ],
   "source": [
    "columns_df1 = set(df.columns)\n",
    "columns_df2 = set(df_kaggle.columns)\n",
    "\n",
    "# Compare the sets of columns\n",
    "if columns_df1 == columns_df2:\n",
    "    print(\"The dataframes have the same column names.\")\n",
    "else:\n",
    "    print(\"The dataframes do not have the same column names.\")\n",
    "\n",
    "    # Find out which columns are different\n",
    "    diff_df1 = columns_df1 - columns_df2\n",
    "    diff_df2 = columns_df2 - columns_df1\n",
    "\n",
    "    if diff_df1:\n",
    "        print(\"Columns in df1 not in df2:\", diff_df1)\n",
    "    if diff_df2:\n",
    "        print(\"Columns in df2 not in df1 (dropped):\", diff_df2)\n",
    "        df_kaggle.drop(diff_df2, axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (17104, 98)\n",
      "y_train shape:  (17104,)\n",
      "X_test shape:  (4277, 98)\n",
      "y_test shape:  (4277,)\n",
      "X_test_kaggle shape:  (24556, 98)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = train.drop(\"price_cleaned\", axis=1)\n",
    "y_train = train[\"price_cleaned\"]\n",
    "\n",
    "X_test = test.drop(\"price_cleaned\", axis=1)\n",
    "y_test = test[\"price_cleaned\"]\n",
    "\n",
    "X_test_kaggle = df_kaggle\n",
    "\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)\n",
    "print(\"X_test_kaggle shape: \", X_test_kaggle.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns\n",
    "\n",
    "X_train_with_na = X_train.copy()\n",
    "X_test_with_na = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=5)\n",
    "X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=cols)\n",
    "X_test_imputed = pd.DataFrame(imputer.transform(X_test), columns=cols)\n",
    "X_test_kaggle_imputed = pd.DataFrame(imputer.fit_transform(df_kaggle), columns=cols)\n",
    "\n",
    "# export imputed train and test data to csv so we dont always have to run the imputation\n",
    "X_train_imputed.to_csv('../../../data/X_train_imputed.csv')\n",
    "X_test_imputed.to_csv('../../../data/X_test_imputed.csv')\n",
    "X_test_kaggle_imputed.to_csv('../../../data/X_test_kaggle_imputed.csv')\n",
    "\n",
    "y_train.to_csv('../../../data/y_train.csv')\n",
    "y_test.to_csv('../../../data/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (17104, 98)\n",
      "y_train shape:  (17104,)\n",
      "X_test shape:  (4277, 98)\n",
      "y_test shape:  (4277,)\n",
      "X_test_kaggle shape:  (24556, 98)\n"
     ]
    }
   ],
   "source": [
    "X_train_imputed = pd.read_csv('../../../data/X_train_imputed.csv', low_memory=False)\n",
    "X_test_imputed = pd.read_csv('../../../data/X_test_imputed.csv', low_memory=False)\n",
    "X_test_kaggle_imputed = pd.read_csv('../../../data/X_test_kaggle_imputed.csv', low_memory=False)\n",
    "y_train = pd.read_csv('../../../data/y_train.csv', low_memory=False)\n",
    "y_test = pd.read_csv('../../../data/y_test.csv', low_memory=False)\n",
    "\n",
    "# ignore first column of X_train\n",
    "X_train_imputed = X_train_imputed.iloc[:, 1:]\n",
    "\n",
    "# ignore first column of X_test\n",
    "X_test_imputed = X_test_imputed.iloc[:, 1:]\n",
    "\n",
    "# ignore first column of X_test_kaggle\n",
    "X_test_kaggle_imputed = X_test_kaggle_imputed.iloc[:, 1:]\n",
    "\n",
    "# ignore first column of y_train\n",
    "y_train = y_train.iloc[:, 1:].values.ravel()\n",
    "\n",
    "# ignore first column of y_test\n",
    "y_test = y_test.iloc[:, 1:].values.ravel()\n",
    "\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)\n",
    "print(\"X_test_kaggle shape: \", X_test_kaggle.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=cols)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=cols)\n",
    "X_test_kaggle = pd.DataFrame(scaler.fit_transform(X_test_kaggle), columns=cols)\n",
    "X_train_with_na = pd.DataFrame(scaler.fit_transform(X_train_with_na), columns=cols)\n",
    "X_test_with_na = pd.DataFrame(scaler.transform(X_test_with_na), columns=cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell 4: XGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dokumentation](https://xgboost.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderschilling/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'booster': 'gbtree', 'learning_rate': 0.2, 'max_depth': 12, 'random_state': 42}\n",
      "MAPE: 0.2432\n",
      "R2: 0.8182\n"
     ]
    }
   ],
   "source": [
    "def xgb_model(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        X_test, \n",
    "        y_test, \n",
    "        cv,\n",
    "        max_depth,\n",
    "        learning_rate,\n",
    "        booster\n",
    "    ):\n",
    "    \n",
    "    xgb = XGBRegressor()\n",
    "\n",
    "    param_grid = {\n",
    "        'max_depth': max_depth,\n",
    "        'learning_rate': learning_rate,\n",
    "        'booster': booster,\n",
    "        'random_state': [42]\n",
    "    }\n",
    "\n",
    "    grid_xgb = GridSearchCV(xgb, param_grid, cv=cv, n_jobs=-1, scoring='neg_mean_absolute_percentage_error', verbose=1)\n",
    "\n",
    "    grid_xgb.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_xgb.best_params_\n",
    "    print('Best parameters: {}'.format(best_params))\n",
    "\n",
    "    y_pred_xgb = grid_xgb.predict(X_test)\n",
    "\n",
    "    mape_xgb = round(mean_absolute_percentage_error(y_test, y_pred_xgb), 4)\n",
    "    print('MAPE: {}'.format(mape_xgb))\n",
    "\n",
    "    r2_xgb = round(r2_score(y_test, y_pred_xgb), 4)\n",
    "    print('R2: {}'.format(r2_xgb))\n",
    "\n",
    "    return grid_xgb, mape_xgb\n",
    "\n",
    "xgb_regression, mape_xgb = xgb_model(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    cv=5,\n",
    "    max_depth = [None, 6, 9, 12, 15],\n",
    "    learning_rate = [0.01, 0.1, 0.2],\n",
    "    booster = ['gbtree']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell 5: XGBoost (Log-Transformiert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderschilling/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'booster': 'gbtree', 'learning_rate': 0.1, 'max_depth': 12, 'random_state': 42}\n",
      "MAPE: 0.2415\n",
      "R2: 0.7411\n"
     ]
    }
   ],
   "source": [
    "def xgb_model(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv,\n",
    "        max_depth,\n",
    "        learning_rate,\n",
    "        booster\n",
    "    ):\n",
    "\n",
    "    xgb = XGBRegressor()\n",
    "\n",
    "    param_grid = {\n",
    "        'max_depth': max_depth,\n",
    "        'learning_rate': learning_rate,\n",
    "        'booster': booster,\n",
    "        'random_state': [42]\n",
    "    }\n",
    "\n",
    "    grid_xgb = GridSearchCV(xgb, param_grid, cv=cv, n_jobs=-1, scoring='neg_mean_absolute_percentage_error', verbose=1)\n",
    "\n",
    "    grid_xgb.fit(X_train, np.log(y_train))\n",
    "\n",
    "    best_params = grid_xgb.best_params_\n",
    "    print('Best parameters: {}'.format(best_params))\n",
    "\n",
    "    y_pred_xgb = grid_xgb.predict(X_test)\n",
    "    y_pred_xgb = np.exp(y_pred_xgb)\n",
    "\n",
    "    mape_xgb = round(mean_absolute_percentage_error(y_test, y_pred_xgb), 4)\n",
    "    print('MAPE: {}'.format(mape_xgb))\n",
    "\n",
    "    r2_xgb = round(r2_score(y_test, y_pred_xgb), 4)\n",
    "    print('R2: {}'.format(r2_xgb))\n",
    "\n",
    "    return grid_xgb, mape_xgb\n",
    "\n",
    "xgb_regression_log, mape_xgb_log = xgb_model(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    cv=5,\n",
    "    max_depth = [None, 6, 9, 12],\n",
    "    learning_rate = [0.01, 0.1, 0.3],\n",
    "    booster = ['gbtree', 'dart']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell 6: Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dokumentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best parameters: {'random_state': 42}\n",
      "MAPE: 0.3354\n",
      "R2: 0.7454\n"
     ]
    }
   ],
   "source": [
    "def rf_model(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        X_test, \n",
    "        y_test, \n",
    "        cv\n",
    "    ):\n",
    "\n",
    "    rf = RandomForestRegressor()\n",
    "\n",
    "    param_grid = {\n",
    "        'random_state': [42]\n",
    "    }\n",
    "\n",
    "    grid_rf = GridSearchCV(rf, param_grid, cv=cv, n_jobs=-1, scoring='neg_mean_absolute_percentage_error', verbose=1)\n",
    "\n",
    "    grid_rf.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_rf.best_params_\n",
    "    print('Best parameters: {}'.format(best_params))\n",
    "\n",
    "    y_pred_rf = grid_rf.predict(X_test)\n",
    "\n",
    "    mape_rf = round(mean_absolute_percentage_error(y_test, y_pred_rf), 4)\n",
    "    print('MAPE: {}'.format(mape_rf))\n",
    "\n",
    "    r2_rf = round(r2_score(y_test, y_pred_rf), 4)\n",
    "    print('R2: {}'.format(r2_rf))\n",
    "\n",
    "    return grid_rf, mape_rf\n",
    "\n",
    "rf_regression, mape_rf = rf_model(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    cv=5,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell 7: Random Forest (Log-Transformiert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "[CV] END ....................................random_state=42; total time=  21.1s\n",
      "[CV] END ....................................random_state=42; total time=  21.3s\n",
      "Best parameters: {'random_state': 42}\n",
      "MAPE: 0.2483\n",
      "R2: 0.7404\n"
     ]
    }
   ],
   "source": [
    "def rf_model(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv,\n",
    "    ):\n",
    "\n",
    "    rf = RandomForestRegressor()\n",
    "\n",
    "    param_grid = {\n",
    "        'random_state': [42],\n",
    "    }\n",
    "\n",
    "    grid_rf = GridSearchCV(rf, param_grid, cv=cv, n_jobs=-1, scoring='neg_mean_absolute_percentage_error', verbose=1)\n",
    "\n",
    "    grid_rf.fit(X_train, np.log(y_train))\n",
    "\n",
    "    best_params = grid_rf.best_params_\n",
    "    print('Best parameters: {}'.format(best_params))\n",
    "\n",
    "    y_pred_rf = grid_rf.predict(X_test)\n",
    "    y_pred_rf = np.exp(y_pred_rf)\n",
    "\n",
    "    mape_rf = round(mean_absolute_percentage_error(y_test, y_pred_rf), 4)\n",
    "    print('MAPE: {}'.format(mape_rf))\n",
    "\n",
    "    r2_rf = round(r2_score(y_test, y_pred_rf), 4)\n",
    "    print('R2: {}'.format(r2_rf))\n",
    "\n",
    "    return grid_rf, mape_rf\n",
    "\n",
    "rf_regression_log, mape_rf_log = rf_model(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    cv=2\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell 8: Hist Gradient Boosting Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dokumentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END l2_regularization=0.1, learning_rate=0.1, loss=gamma, max_depth=None, max_iter=300, max_leaf_nodes=70, random_state=27; total time=   1.9s\n",
      "[CV] END l2_regularization=0.1, learning_rate=0.1, loss=gamma, max_depth=None, max_iter=300, max_leaf_nodes=70, random_state=27; total time=   1.5s\n",
      "[CV] END l2_regularization=0.1, learning_rate=0.1, loss=gamma, max_depth=None, max_iter=300, max_leaf_nodes=70, random_state=27; total time=   2.1s\n",
      "[CV] END l2_regularization=0.1, learning_rate=0.1, loss=gamma, max_depth=None, max_iter=300, max_leaf_nodes=70, random_state=27; total time=   1.9s\n",
      "[CV] END l2_regularization=0.1, learning_rate=0.1, loss=gamma, max_depth=None, max_iter=300, max_leaf_nodes=70, random_state=27; total time=   2.8s\n",
      "Best parameters: {'l2_regularization': 0.1, 'learning_rate': 0.1, 'loss': 'gamma', 'max_depth': None, 'max_iter': 300, 'max_leaf_nodes': 70, 'random_state': 27}\n",
      "MAPE: 0.2793\n",
      "R2: 0.7515\n"
     ]
    }
   ],
   "source": [
    "def hgb_model(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        X_test, \n",
    "        y_test, \n",
    "        cv, \n",
    "        loss,\n",
    "        learning_rate, \n",
    "        max_iter,\n",
    "        max_leaf_nodes,\n",
    "        max_depth, \n",
    "        l2_regularization, \n",
    "        random_state,\n",
    "    ):\n",
    "    \n",
    "    hgb = HistGradientBoostingRegressor()\n",
    "\n",
    "    param_grid = {\n",
    "        'loss': loss,\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_iter': max_iter,\n",
    "        'max_leaf_nodes': max_leaf_nodes,\n",
    "        'max_depth': max_depth,\n",
    "        'l2_regularization': l2_regularization,\n",
    "        'random_state': random_state\n",
    "    }\n",
    "\n",
    "    grid_hgb = GridSearchCV(hgb, param_grid, cv=cv, n_jobs=-1, scoring='neg_mean_absolute_percentage_error', verbose=1)\n",
    "\n",
    "    grid_hgb.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_hgb.best_params_\n",
    "    print('Best parameters: {}'.format(best_params))\n",
    "\n",
    "    y_pred_hgb = grid_hgb.predict(X_test)\n",
    "\n",
    "    mape_hgb = round(mean_absolute_percentage_error(y_test, y_pred_hgb), 4)\n",
    "    print('MAPE: {}'.format(mape_hgb))\n",
    "\n",
    "    r2_hgb = round(r2_score(y_test, y_pred_hgb), 4)\n",
    "    print('R2: {}'.format(r2_hgb))\n",
    "\n",
    "    return grid_hgb, mape_hgb\n",
    "\n",
    "hgb_regression, mape_hgb = hgb_model(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    cv=5,\n",
    "    loss = ['gamma'],\n",
    "    learning_rate = [0.1],\n",
    "    max_iter = [300],\n",
    "    max_leaf_nodes = [70],\n",
    "    max_depth = [None],\n",
    "    l2_regularization = [0.1],\n",
    "    random_state = [27]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell 9: Hist Gradient Boosting Regression (Log-Transformiert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Best parameters: {'l2_regularization': 0.1, 'learning_rate': 0.1, 'loss': 'gamma', 'max_depth': None, 'max_iter': 300, 'max_leaf_nodes': 70, 'random_state': 27}\n",
      "MAPE: 0.2225\n",
      "R2: 0.79\n"
     ]
    }
   ],
   "source": [
    "def hgb_model(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        X_test, \n",
    "        y_test, \n",
    "        cv, \n",
    "        loss,\n",
    "        learning_rate, \n",
    "        max_iter,\n",
    "        max_leaf_nodes,\n",
    "        max_depth, \n",
    "        l2_regularization, \n",
    "        random_state,\n",
    "    ):\n",
    "    \n",
    "    hgb = HistGradientBoostingRegressor()\n",
    "\n",
    "    param_grid = {\n",
    "        'loss': loss,\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_iter': max_iter,\n",
    "        'max_leaf_nodes': max_leaf_nodes,\n",
    "        'max_depth': max_depth,\n",
    "        'l2_regularization': l2_regularization,\n",
    "        'random_state': random_state\n",
    "    }\n",
    "\n",
    "    grid_hgb = GridSearchCV(hgb, param_grid, cv=cv, n_jobs=-1, scoring='neg_mean_absolute_percentage_error', verbose=1)\n",
    "\n",
    "    grid_hgb.fit(X_train, np.log(y_train))\n",
    "\n",
    "    best_params = grid_hgb.best_params_\n",
    "    print('Best parameters: {}'.format(best_params))\n",
    "\n",
    "    y_pred_hgb = grid_hgb.predict(X_test)\n",
    "    y_pred_hgb = np.exp(y_pred_hgb)\n",
    "\n",
    "    mape_hgb = round(mean_absolute_percentage_error(y_test, y_pred_hgb), 4)\n",
    "    print('MAPE: {}'.format(mape_hgb))\n",
    "\n",
    "    r2_hgb = round(r2_score(y_test, y_pred_hgb), 4)\n",
    "    print('R2: {}'.format(r2_hgb))\n",
    "\n",
    "    return grid_hgb, mape_hgb\n",
    "\n",
    "hgb_regression_log, mape_hgb_log = hgb_model(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    cv=5,\n",
    "    loss = ['gamma'],\n",
    "    learning_rate = [0.1],\n",
    "    max_iter = [300],\n",
    "    max_leaf_nodes = [70],\n",
    "    max_depth = [None],\n",
    "    l2_regularization = [0.1],\n",
    "    random_state = [27]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best parameters: {'l2_regularization': 0.1, 'learning_rate': 0.1, 'loss': 'gamma', 'max_depth': None, 'max_iter': 300, 'max_leaf_nodes': 70, 'random_state': 20}\n",
      "MAPE: 0.2145\n",
      "R2: 0.8335\n"
     ]
    }
   ],
   "source": [
    "hgb_regression_log, mape_hgb_log = hgb_model(\n",
    "    X_train_with_na,\n",
    "    y_train,\n",
    "    X_test_with_na,\n",
    "    y_test,\n",
    "    cv=5,\n",
    "    loss = ['gamma'],\n",
    "    learning_rate = [0.1],\n",
    "    max_iter = [300],\n",
    "    max_leaf_nodes = [70],\n",
    "    max_depth = [None],\n",
    "    l2_regularization = [0.1],\n",
    "    random_state = [20]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell 10: Gradient Boosting Regression\n",
    "\n",
    "[Dokumentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best parameters: {'learning_rate': 0.1, 'max_depth': 9, 'random_state': 42}\n",
      "MAPE: 0.2863\n",
      "R2: 0.7561\n"
     ]
    }
   ],
   "source": [
    "def gb_model(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv,\n",
    "        max_depth,\n",
    "        learning_rate,\n",
    "    ):\n",
    "\n",
    "    gb = GradientBoostingRegressor()\n",
    "\n",
    "    param_grid = {\n",
    "        'max_depth': max_depth,\n",
    "        'learning_rate': learning_rate,\n",
    "        'random_state': [42]\n",
    "    }\n",
    "\n",
    "    grid_gb = GridSearchCV(gb, param_grid, cv=cv, n_jobs=-1, scoring='neg_mean_absolute_percentage_error', verbose=1)\n",
    "\n",
    "    grid_gb.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_gb.best_params_\n",
    "    print('Best parameters: {}'.format(best_params))\n",
    "\n",
    "    y_pred_gb = grid_gb.predict(X_test)\n",
    "\n",
    "    mape_gb = round(mean_absolute_percentage_error(y_test, y_pred_gb), 4)\n",
    "    print('MAPE: {}'.format(mape_gb))\n",
    "\n",
    "    r2_gb = round(r2_score(y_test, y_pred_gb), 4)\n",
    "    print('R2: {}'.format(r2_gb))\n",
    "\n",
    "    return grid_gb, mape_gb\n",
    "\n",
    "gb_regression, mape_gb = gb_model(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    cv=5,\n",
    "    max_depth = [None, 3, 6, 9],\n",
    "    learning_rate = [0.01, 0.1],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell 11: Gradient Boosting Regression (Log-Transformiert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderschilling/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.1, 'max_depth': 9, 'random_state': 42}\n",
      "MAPE: 0.2438\n",
      "R2: 0.7498\n"
     ]
    }
   ],
   "source": [
    "def gb_model(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv,\n",
    "        max_depth,\n",
    "        learning_rate\n",
    "    ):\n",
    "\n",
    "    gb = GradientBoostingRegressor()\n",
    "\n",
    "    param_grid = {\n",
    "        'max_depth': max_depth,\n",
    "        'learning_rate': learning_rate,\n",
    "        'random_state': [42]\n",
    "    }\n",
    "\n",
    "    grid_gb = GridSearchCV(gb, param_grid, cv=cv, n_jobs=-1, scoring='neg_mean_absolute_percentage_error', verbose=1)\n",
    "\n",
    "    grid_gb.fit(X_train, np.log(y_train))\n",
    "\n",
    "    best_params = grid_gb.best_params_\n",
    "    print('Best parameters: {}'.format(best_params))\n",
    "\n",
    "    y_pred_gb = grid_gb.predict(X_test)\n",
    "    y_pred_gb = np.exp(y_pred_gb)\n",
    "\n",
    "    mape_gb = round(mean_absolute_percentage_error(y_test, y_pred_gb), 4)\n",
    "    print('MAPE: {}'.format(mape_gb))\n",
    "\n",
    "    r2_gb = round(r2_score(y_test, y_pred_gb), 4)\n",
    "    print('R2: {}'.format(r2_gb))\n",
    "\n",
    "    return grid_gb, mape_gb\n",
    "\n",
    "gb_regression_log, mape_gb_log = gb_model(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    cv=5,\n",
    "    max_depth = [None, 3, 6, 9],\n",
    "    learning_rate = [0.01, 0.1]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Datensatz Predictions\n",
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_regression.predict(X_test_kaggle)\n",
    "\n",
    "predictions = pd.DataFrame({\"ID\": indexes, \"Expected\": y_pred})\n",
    "\n",
    "predictions.to_csv('../../99_gespeicherte_modelle/predictions_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_regression_log.predict(X_test_kaggle)\n",
    "\n",
    "y_pred = np.exp(y_pred)\n",
    "\n",
    "predictions = pd.DataFrame({\"ID\": indexes, \"Expected\": y_pred})\n",
    "\n",
    "predictions.to_csv('../../99_gespeicherte_modelle/predictions_xgb_log.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = re_regression.predict(X_test_kaggle)\n",
    "\n",
    "predictions = pd.DataFrame({\"ID\": indexes, \"Expected\": y_pred})\n",
    "\n",
    "predictions.to_csv('../../99_gespeicherte_modelle/predictions_rf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_regression_log.predict(X_test_kaggle)\n",
    "\n",
    "y_pred = np.exp(y_pred)\n",
    "\n",
    "predictions = pd.DataFrame({\"ID\": indexes, \"Expected\": y_pred})\n",
    "\n",
    "predictions.to_csv('../../99_gespeicherte_modelle/predictions_rf_log.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hist Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = hgb_regression.predict(X_test_kaggle)\n",
    "\n",
    "predictions = pd.DataFrame({\"ID\": indexes, \"Expected\": y_pred})\n",
    "\n",
    "predictions.to_csv('../../99_gespeicherte_modelle/predictions_hgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = hgb_regression_log.predict(X_test_kaggle)\n",
    "\n",
    "y_pred = np.exp(y_pred)\n",
    "\n",
    "predictions = pd.DataFrame({\"ID\": indexes, \"Expected\": y_pred})\n",
    "\n",
    "predictions.to_csv('../../99_gespeicherte_modelle/predictions_hgb_log.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gb_regression.predict(X_test_kaggle)\n",
    "\n",
    "predictions = pd.DataFrame({\"ID\": indexes, \"Expected\": y_pred})\n",
    "\n",
    "predictions.to_csv('../../99_gespeicherte_modelle/predictions_gb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/7lm0cqd933xgjvldmnsjpphh0000gn/T/ipykernel_51121/3154516622.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  y_pred = np.exp(y_pred)\n"
     ]
    }
   ],
   "source": [
    "y_pred = gb_regression_log.predict(X_test_kaggle)\n",
    "\n",
    "y_pred = np.exp(y_pred)\n",
    "\n",
    "predictions = pd.DataFrame({\"ID\": indexes, \"Expected\": y_pred})\n",
    "\n",
    "predictions.to_csv('../../99_gespeicherte_modelle/predictions_gb_log.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd9c78cbb62892232a2e8cf9a4bd699d988202e949c50bb9be5232199c394801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
